Project implements feedforward propagation and backpropagation for a neural network used to predict hanwritten digits.
Neural network has 3 layers - an input layer, a hidden layer and an output layer.
Dataset is obtained from ex4data1.mat file that contains 5000 training examples of handwritten digits.
Each training example is a 20 pixel by 20 pixel grayscale image of the digit. Each pixel is represented by a floating point 
number indicating the grayscale intensity at that location. The 20 by 20 grid of pixels is unrolled into a 400-dimensional vector.
Each of these training examples becomes a single row in data matrix X. 

Network parameters were provided by Stanford university in ex4weights.mat and will be loaded by ex4.m into Theta1 and Theta2.
Most calculations are handled by nnCostFunction.m. 

First block performs feedforward and cost function calculation using the loaded
set of parameters for Theta1 and Theta2.

Second block performs backpropagation algorithm to compute the gradient for the neural network cost function. 
Cost function is minimized with an advanced optimizer fmincg.m. Sigmoid gradient function is implemented in sigmoidGradient.m
Initial neural network parameters are randomly generated by randInitializeWeights.m. After that feedforward and backpropagation for 
provided dataset are implemented. Project also performs gradient checking in checkNNGradients.m by comparing results with manually 
calculated approximate gradient in computeNumericalGradient.m
